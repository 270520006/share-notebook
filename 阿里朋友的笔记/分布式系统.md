







# 分布式系统



**集群：**

不同服务器部署同一套应用服务对外提供访问，实现服务的负载均衡或者互备（热备、主从等），指同一种组件的多个实例，形成的逻辑上的整体。

单个节点可以提供完整服务。集群是物理形态。



**分布式：**

服务的不同模块部署在不同的服务器上，单个节点不能提供完整服务，需要多节点协调提供服务（也可以是相同组件部署在不同节点、节点间通过交换信息协作提供服务），分布式强调的是工作方式。



**SOA：**

面向服务的架构，一种设计方法，其中包含多个服务，服务之间通过相互依赖最终提供一系列的功能。一个服务通常以独立的形式存在于操作系统进程中。各个服务通过网络调用。

- 中心化实现：ESB（企业服务总线），各服务通过 ESB 进行交互，解决异构系统之间的连通性，通过系统之间的连通性，通过协议转换、消息解析、消息路由把服务提供者的数据传送到服务消费者。很重，有一定的逻辑，可以解决一些公用逻辑的问题。
- 去中心化实现：微服务



**微服务：**

在 SOA 上做的升华，微服务架构强调的一个重点是业务需要彻底的组件化和服务化，原有的单个业务系统会拆分成多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成。

服务单一职责

轻量级通信：去掉 ESB 总线，采用 restapi 通信。





### CAP 理论

C 强一致性（consistency）：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，所有读请求都不能读到这个数据，对调用者而言数据具有强一致性（strong consistency）

A 高可用（availability）：所有读写请求在一定时间内得到响应，可终止、不会一直等待。

P 分区容错性（partition-tolerance）：在网络分区的情况下，被分割的节点仍能正常对外服务。



如果选择了 CA 放弃 P，当发生网络分区时，为了保证 C，系统需要禁止写入，当有写入请求时，系统但会 error（例如当前系统不允许写入），这又和 A 冲突了，因为 A 要求返回 no error 和 no timeout，因此，分布式系统理论

不可能选择 CA 架构，只能选择 CP 或 AP 架构



> 一致性

- 强一致性：集群中数据同步完才能接收 client 
- 弱一致性：集群中同步数据的功能可有可没有
- 最终一致性：集群中同步数据的功能必须有，可以在没同步完数据的情况下接收 client 的请求 

### Base 理论

base 理论是 cap 理论的一种妥协，由于 cap 之能二取其一，base降低了发生分区容错时对高可用和一致性的要求

1. 基本可用：允许可用性降低（可能响应延时、可能服务降级）
2. 软状态：指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性
3. 最终一致性：节点数据同步可以存在时延，但在一定的期限后必须达到数据的一致，状态变为最终状态。





### Quorum、WARO 机制

选举算法 Quorum 机制：

​	10 个副本，一次成功更新了 3 个，那么至少需要读取 8 个副本的数据，可以保证读到了最新的数据。无法保证强一致性，也就是无法实现任何用户或节点都可以读到最近一次成功提交的副本数据。需要配合一个获取最新成功提交的版本号的 metadata 服务，这样可以确定最新已经成功提交的版本号，然后从已经读到的数据中就可以确认最新写入的数据。

​	

waro ：

​	一种简单的副本控制协议，写操作时、只有当所有的副本都更新成功后，这次写操作才算成功，否则视为失败。优先保证读、任何节点读到的数据都是最新数据，牺牲了更新服务的可用性、只要有一个副本宕机了，写服务就不会成功。但只要有一个节点存活、仍能提供读服务。







### paxos 算法模型

Paxos 算法（共识算法）解决的是一个分布式系统如果就某个值达成一致





- Proposer 提议者：只要 proposer 发的题案 propose 被半数以上的 Acceptor 就认为该提案的 value 被选中。
- Acceptor 接受者：只要 Acceptor 接受了某个题案，Acceptor 就认为该题案的 value 被选中了
- Learner 记录员：Acceptor 告诉 Learner 哪个 value 被选中，Learner 就认为哪个 value 被选中。



paxos 分为两个阶段：准备阶段 和 接受阶段

阶段一 prepare：

（a）Proposer 收到 client 请求或者发现本地有未提交的值，选择一个题案编号 N，然后向半数以上的 Acceptor 发送编号为 N 的 prepare 请求

（b）Acceptor 收到一个编号为 N 的 Prepare 请求，如果是同一轮的 paxos

- 本节点已经有已提交的 value 记录，对比记录的编号 和 N，大于 N 则拒绝回应，否则返回该记录 value 及编号
- 没有已提交记录，判断本地是否有编号 N1，N1 > N，则拒绝响应，否则将 N1 改为 N（如果没有 N1，则记录 N），并响应 prepare 	

阶段二 accept：



### 负载均衡策略

> 1、轮询法

将请求按顺序轮流的分配到后端服务器上，它均衡的对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载



> 2、加权轮询法

不同的后端服务机器可能机器的配置和当前系统的负载并不相同，因此抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请求，而配置低、负载高的机器，给其分配较低的权重，降低其系统负载。

加权轮询能很好处理这一问题，并将请求顺序按照权重分配到后端



> 3、随机法

通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果



> 4、加权随机法

与加权轮询法一样，加权随机法也根据后端机器的配置，系统的分配不同的权重。不同的是，它是按照权重随即请求后端服务器，而非顺序。

比如后端服务器有 A、B、C，A 的性能好，那么 A、A虚拟节点、B、C，这样 A 被抽查到的概率大大增加



> 5、源地址哈希法

源地址哈希的思想是根据 client 的 ip 地址，通过哈希函数计算得到数值,用该数值对服务器列表带大小进行取模运算,得到的结果便是客户端要访问服务器的序号。

session 共享问题：采用源地址哈希进行负载均衡，同一 ip 地址的客户端（**ip 不变**），当后端服务列表不变时，每次都会映射到同一台后端服务器进行访问

缺点：割裂了集群，就是每个后端服务器都单独服务固定的 client，而且 ip 地址也会改变。



> 6、最小连接数法

最小连接数算法比较灵活智能，由于后端服务器的配置不尽相同，对于请求处理有快有慢，它是根据后端服务器的当前连接情况，动态的选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用率，将负责合理的分流到每一台服务器。





### 分布式系统的设计目标

- 可扩展性：通过对服务、存储的扩展，来提高系统的处理能力，通过对多台服务器协同工作，来完成单台服务器无法处理的任务，尤其是高并发或者大数据量的任务。
- 高可用：单点不影响整体，单点故障指系统中某个组件一旦失效，会让整个系统无法工作。
- 无状态：无状态的服务才能满足部分机器宕机不影响全部，可以随时进行扩展的需求
- 可管理：便于运维，出问题能不能及时发现定位
- 高可靠：同样的请求返回同样的数据；更新能够持久化；数据不会丢失









### 如何解决接口幂等性



要求是支付一个订单，必须插入一条支付流水，order_id 建一个唯一键，unique key

所以你在支付一个订单之前，先插入一条支付流水，order_id 就已经进去了，

可以写把 order_id 写到 redis 里面，下一次重复请求过来了，先查 redis 的 order_id 对应的 value，如果是 payed 说明已经支付过了

然后，你再重复支付这个订单的时候，你尝试插入一条支付流水，数据库给你报错了，说明 unique key 冲突，整个事务可以回滚



### 如何保证服务接口请求的顺序性



![分布式1](F:\自我总结qwq\消息队列面试题\照片\分布式1.png)





**可以利用 分布式锁 + MySQL 来保证 100% 的顺序性，不建议这样做，耗费性能**





## Zookeeper 的应用



#### **分布式协调：**

经典用法：A系统发送个请求到 mq，然后 B消息消费之后处理了，那么 A系统如何知道 B系统的处理结果？

​	用 zk 就可以实现分布式系统之间的协调工作。A系统发送请求之后可以在 zk 上对某个节点的值注册监听器，一旦 B系统处理完了就修改 zk 那个节点的值，A 就可以立马收到通知，完美解决。	





#### 分布式锁：

**临时节点：**	

​	对某一个数据连续发出两个修改操作，两台机器A、B 同时收到了请求，但是只能一台机器先执行另外一个机器再执行。那么此时就可以使用 zk 分布式锁，     机器A 接收到了请求之后先获取 zk 上的一把分布式锁，就是可以去创建一个 znode（临时节点），接着执行操作，机器B 也尝试去创建那个 znode，结果发现自己创建不了，然后机器B 对这个锁设置监听器，当机器A 执行完删除 znode，机器B 设置的监听器就会感知到，此时机器B 去创建 znode 执行自己的业务逻辑 。

- znode 必须是临时节点，防止机器A 挂掉之后，节点无法删除，类似死锁。



代码更优雅：使用临时顺序节点

**临时顺序节点：感觉借鉴了 AQS 的思想**

系统A、B、C：创建临时顺序节点   /locks/000000、/locks/000001、/locks/000002

系统A 先拿到锁会执行，后面的每个系统都会去监听排在自己前面那个系统创建的 znode ，系统B 监听系统A、系统C 监听系统B ，一旦系统A 释放了锁，排在系统A 后面的系统B 就会被 zookeeper 通知，一旦接收到了通知，那么就ok，自己就获取到了锁，就可以执行代码了。

- 系统B、系统C 获取不到锁进行阻塞



#### 元数据/配置信息管理

​	zk 可以用作很多系统的配置信息的管理，比如 kafka、storm 等很多分布式系统都会选用 zk 来做一些元数据、配置信息的管理,包括 dubbo 注册中心也支持 zk

![分布式3](F:\自我总结qwq\消息队列面试题\照片\分布式3.png)



#### HA 高可用性

​	这个是很常见的，比如 hadoop、 hdfs、yarn 等很多大数据系统，都选择基于 zk 来开发 HA 高可用机制，就是一个重要进程一般都会做主备两个，主进程挂了立马通过 zk 感知到切换到备用进程。



![分布式2](F:\自我总结qwq\消息队列面试题\照片\分布式2.png)







## 分布式锁 zk or redis



redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能

zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小



另外一点就是，redis 获取锁的那个客户端出 bug 了或挂了，那么只能等待超时时间之后才能释放锁；

而 zk 的话，因为创建的是 临时znode ，只要客户端挂了，znode 就没了，此时自动释放锁 。



redis 分布式锁大家发现好麻烦：遍历上锁、计算时间等.....   zk 的分布式锁语义清晰，实现简单





## 分布式 Session 

单机 session 中的数据都放在 tomcat 容器中，在分布式下就不能进行共享



### **一、tomcat + redis**（重耦合）

​	基于 tomcat 原生的 session 支持即可，然后用一个叫做 TomcatRedisSessionManager 的东西，让所有我们部署的 tomcat 都将 session 数据存储到 redis。

​	这是在 tomcat 中的配置文件中进行配置，然后进行查看是否配置成功。

```
<valve className="com.orangefuncation.tomcat.redissions.RedisSessionHandlerValve" />

<Manager className="com.orangefuncation.tomcat.redissions.RedisSessionManager"
	host="{redis.host}"
	port="{redis.port}"
	database="{redis.dbnum}"
	maxInactiveInterval="60" />
```





### **二、spring session + redis**













## 分布式事务



### 一、两阶段提交方案 / XA 方案

此方案有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问各个数据库你准备好了吗？如果每个数据库都回复 ok，那么就正式提交事务，在各个数据库上执行操作，只要有一个数据库回答不 ok，那么就回滚事务。

![分布式14](F:\自我总结qwq\消息队列面试题\照片\分布式14.png)

**超时机制：针对于事务管理器**

TM 发送 prepare 给数据库，其中一个数据库在规定时间内没有返回成功 or 失败的消息，那么就回滚事务



问题：

- 单点故障：事务管理器宕机，整个系统不可用
- 数据不一致：在阶段二，此时事务管理器发送了部分 commit ，此时网络异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。
  - 不知道数据库是否宕机，直接发送数据请求。
- 响应时间长：参与者与协调者资源都被锁住，提交或回滚之后才能释放
- 不确定性：当事务管理器发送 commit 之后，并且此时只有一个参与者收到了 commit，那么当该参与者与事务管理器同时宕机后，重新选举的事务管理器无法确定该条消息是否提交成功。





这种分布式事务方案，比较适合单块应用中，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率低，绝对不适合高并发的场景。

如果要使用这个方案：spring + JTA

​	

这个方案很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。

现在微服务，一个大的系统分成几百个服务，几十个服务。一般来说我们的规定和规范，是要求每个服务只能操作自己对应的一个数据库，如果你要操作别的服务对应的库，不允许直连别的服务的库，违反微服务架构的规范。



### 二、三阶段提交方案 / XA 方案

三阶段协议：主要是针对两阶段提交方案的优化，解决了 2PC 单点故障的问题，但是性能问题和不一致问题仍然没有根本解决。



![分布式15](F:\自我总结qwq\消息队列面试题\照片\分布式15.png)



**超时机制：针对于数据库**



### 三、TCC 方案（实现最复杂）

TCC 的全称：Try、Confirm、Cancel

这个其实用到了补偿的概念，分为了三个阶段

Try 阶段：先把两个银行账户中的资金给它冻结就不让操作了

Confirm阶段：这个阶段说的是在各个服务中执行实际的操作

Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚。



**缺点：**

- TCC 模型对业务的侵入性特别强，每个操作都需要有 try、confirm、cancel 三个接口实现。

- 事务回滚严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。
- TCC 会添加事务日志，如果 Confirm 或 Cancel 阶段出错，则会进行重试，所以这两个阶段需要支持幂等；如果重试失败，则需要人工介入进行恢复何处理。

**适用场景：**

​	这个就是你要求强一致性，是你系统中核心之核心的场景，比如常见的就是资金类的场景，那么你就可以用 TCC 方案了，自己编写大量的业务逻辑，自己判断一个事务中的各个环节是否 ok，不 ok 就执行补偿 / 回滚代码。而且最好是你的各个业务执行的时间都比较短。

但是说实话，一般尽量别这么搞，自己手写回滚逻辑或者是补偿机制，实在恶心，业务代码也很难维护。





### 四、本地消息表



1）A 系统在自己本地事务里操作的同时插入一条数据到消息表

2）A 将消息发送到 MQ

3）B 系统接收到消息，在一个事务里，往自己的本地消息表插入一条数据，同时执行其他的业务，如果这个消息已经被处理过了，那么此时这个事务会回滚，这样保证不会重复消费这个消息

4）B 系统执行成功后，就会更新在自己本地消息表状态以及 A 系统消息表的状态

5）B 系统执行失败，就不会更新消息表状态，此时 A 系统会定时扫描自己的消息表，如果有没处理的消息，会再次发送到 MQ 中，让 B 再次处理

6）这个方案保证了最终一致性，哪怕 B 事务失败了，但是 A 会不断重发消息，直到 B 成功为止。



**缺点：**

​	这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的？？？

​	如果是高并发场景下，怎么进行扩展？？？所以一般很少用



![分布式4](F:\自我总结qwq\消息队列面试题\照片\分布式4.png)



### 五、可靠消息最终一致性方案



干脆不要用本地消息表了，直接基于 MQ 来实现事务，比如阿里的 RocketMQ 就支持消息事务。

1. A 系统先发送一个 prepared 消息到 mq，如果这个 prepared 消息发送失败那么就直接取消操作不执行
2. 如果这个消息发送成功，那么 A 执行本地事务，执行成功就告诉 mq 发送确认消息，执行失败就告诉 mq 回滚消息
3. 如果发送了确认消息，此时 B 系统接收到确认消息，然后执行本地事务
   - 系统B 保证自己的幂等性
4. mq 会自动定时轮询所有 prepared 消息回调的接口，问你，这个消息是不是本地事务处理失败了，所以没发送确认消息？是继续重试还是回滚？一般来说在这里你就可以查下数据库看之前的本地事务是否执行了，回调 A 系统的一个接口，问 A 系统是继续重发还是回滚？如果回滚了，那么这里也回滚，这个就是避免可能本地事务执行成功了，确认消息发送失败了。
5. 这个方案，要是系统 B 的事务失败了咋办？重试，自动不断重试直到成功，如果实在不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚，或者发送警报由人工来手工回滚和补偿



目前国内互联网公司大部分都是这么玩的，或者你基于 RabbitMQ 自己封装一套类似的逻辑出来，总之思路就是这样



![分布式6](F:\自我总结qwq\消息队列面试题\照片\分布式6.png)



### 六、最大努力通知方案

1. 系统A 本地事务执行完之后，发送个消息到 MQ
2. 这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列，接着调用系统B 的接口
3. 要是系统B 执行成功就 ok，要是系统B 执行失败，那么最大努力通知服务就定时重新调用系统B，反复 N 次，最后还是不行就放弃。



特点：

​	可以在一定程度上允许少数的分布式事务失败，一般用在对分布式事务要求不严格的情况下，比如说记录个日志或者是状态。





![分布式5](F:\自我总结qwq\消息队列面试题\照片\分布式5.png)







### 总结



**TCC 方案**：资金要求绝对不能错的场景

**可靠消息最终一致性方案**：一般的分布式系统，订单插入之后要调用库存服务更新库存，库存数据没有资金那么敏感。



**友情提示**：

​	rocketMQ 3.2.6 之前的版本，是可以按照上面的思路来玩的，因为支持定时轮询 prepared 回调接口，但是之后接口做了一些改变，可以去看官网。



**一个问题：**

​	我们现在想保证我们某个系统非常的可靠，任何一个数据都不能错，是一个微服务架构，几十个服务，结果一盘点发现，如果要搞分布式事务的话，一个系统要搞几十个分布式事务出来。



一个几百个服务的分布式系统，里面其实也没几个分布式事务。

- 用任何一个分布式事务方案，都会使你的那块代码复杂 10 倍
- 很多情况下，系统A 调用系统B、系统C 、系统D，可能都不会做分布式事务。如果调用报错会打印异常日志。
- 如果你为了确保系统自动宝成数据 100% 不出错，上了几十个分布式事务，代码复杂，性能太差，系统吞吐量差，性能大幅度下降。



99% 的分布式接口调用，不要做分布式事务，直接就是：

监控（发邮件、发短信）、记录日志（一旦出错，完整的日志）、事后快速的定位、排查和查出解决方案、修复数据









## 如何设计一个高并发的系统架构？



### 一、系统拆分

​	将一个系统拆分为多个子系统，用 dubbo 来搞，然后每个系统连一个数据库，这样本来就一个库，现在有多个数据库，也可以抗高并发

### 二、缓存

​	必须得用缓存。大部分的高并发场景，都是读多写少，redis 轻轻松松单机几万的并发，完全没问题，所以可以考虑考虑你的项目里，哪些承载主要请求的读场景，怎么用缓存来抗高并发。

### 三、MQ

​	必须得用 MQ。可能你还是会出现高并发写的场景，比如一个业务操作里要频繁搞数据库几十次，增删改增删改，疯了。高并发绝对搞垮你的系统，你要是用 redis 来承载写那肯定不行，人家是缓存，数据随时被 LRU，数据格式还简单，没有事务支持。

​	大量的写请求灌入 MQ，排队慢慢来，后边系统慢慢消费，控制在 MySQL 承载范围之内，所以你得考虑你的项目里，哪些承载复杂写业务逻辑的场景，如何用 MQ 来异步写，提升并发性。MQ 单机抗几万并发也是 ok

### 四、分库分表

​	可能到了最后数据库层面还是免不了抗高并发的请求，就将一个数据库拆分为多个库，多个库来抗更高的并发，然后将一个表拆分为多个表，每个表的数据量保持少一点，提高 sql 运行的性能。

### 五、读写分离

​	大部分数据库也是读多写少，没必要所有请求都集中在一个库上，可以搞个主从架构，主库写入，从库读取，读流量多的时候，还可以多加从库

### 六、Elasticsearch

​	可以考虑用 es，es是分布式的，可以随便扩容。分布式天然支持高并发，动不动就可以扩容加机器来抗更高的并发，一些简单的查询、统计类的操作，可以考虑用 es 来承载，还有一些全文搜索类的操作。







## 分库分表

分库分表肯定是设计到了高并发，因为分库分表一定是为了支持高并发、数据量大两个问题。

分库分表方案更多的是对关系型数据库数据存储和访问机制的一种补充，而不是颠覆。



#### 一.为什么要分库分表

​	设计高并发系统的时候，数据库层面该如何设计？

​	分库分表是两回事，可能是只分库不分表，也可能是只分表不分库。

![分布式7](F:\自我总结qwq\消息队列面试题\照片\分布式7.png)





**用过哪些分库分表中间件？**

目前流行的：sharding-jdbc、mycat

sharding-jdbc：这种 client 层方案的优点在于不用部署，运维成本低，不需要 proxy 的二次转发请求，性能很高，但是如果遇到升级啥的需要各个系统都重新升级版本再发布；

mycat：这种 proxy 层方案的优点在于需要部署，自己及运维一套中间件，运维成本高，但是好处在于对各个项目是透明的，如果遇到升级之类的都是自己中间件那里搞就可以了。



通常来说，这两个方案都可以使用，个人建议中小公司选用 sharding-jdbc，client 层方便轻巧，维护成本低，不需要额外增派人手，而且中小公司系统复杂度会低一些，项目也没那么多。

中大型公司最好还是选用 mycat 这类 proxy 层方案，大公司系统和项目非常多，团队大，人员充足，专门弄个人研究和维护 mycat，然后大量项目直接透明使用即可。





![分布式8](F:\自我总结qwq\消息队列面试题\照片\分布式8.png)





**你们具体是如何对数据库如何进行垂直拆分或水平拆分的？**

**水平拆分：**

​	把一个表的数据给分到多个库的多个表中，每个库中的表结构都一样，只不过每个库中的数据是不同的，所有库表的数据加起来就是全部数据。

**意义：**

​	将数据均匀放到更多的库中，然后多个库来抗更高的并发，还有就是用多个库的存储容量来进行扩容

**问题：**

​	不要采用 hash 来处理 id，以后进行扩展增加数据库时，要把之前的数据全部取出来，重新 hash 重新入库，这个分配策略十分垃圾。

水平拆分特点：

​	1、每个库（表）的结构都一样

​	2、每个库（表）的数据都不一样

​	3、每个库（表）的并集是全量数据

优点：

​	1、单库（表）的数据保持在一定的量（减少），有助于性能提高

​	2、提高了系统的稳定性和负载能力

​	3、拆分表结构相同、程序改造较少

缺点：

​	1、数据的扩容很有难度维护量大

​	2、拆分规则很难抽取出来

​	3、分片事务的一致性问题、







**垂直拆分**：

​	把一个有很多字段的表拆分成多个表，或者拆分到多个库中去，每个库中的表结构都不一样，每个库表都包含部分字段。

​	一般来说，会将较少的访问频率很高的字段放到一张表中，然后将较多的访问频率很低的字段放到另外一张表中，因为数据库是有缓存的，你访问频率高的行字段越少，就可以在缓存里缓存更多的行，性能就越好。这个一般在表层面做的较多一些。

垂直拆分特点：

​	1、每个库（表）的结构都不一样

​	2、每个库（表）的数据至少有一列一样，进行关联

​	3、每个库（表）的并集是全量数据

优点：

​	1、拆分后业务清晰（转库专用业务拆分）

​	2、数据维护简单、按业务不同放在不同机器上

缺点：

​	1、如果单表的数据量大、写读压力大

​	2、受某种业务来决定或限制。也就是说一个业务往往会影响到数据库的瓶颈（性能问题）

​	3、部分业务无法关联 join，只能用java 程序接口去调用，提高了开发复杂度





**表层面的拆分：**

​	就是分表，将一个表变成 N 个表，就是让每个表的数据量控制再一定范围内，保证 SQL 性能，否则单表数据量越大，SQL 性能就越差。一般是 200 万行左右，不要太多。





#### 二.系统动态切换到分库表上？

**现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库表上？**

​	现在你已经知道为什么要分库分表了，也知道常用的分库分表中间件了，你也设计好如何分库分表的方案了（水平拆分、垂直拆分、分表），那问题来了，接下来怎么把你那个单库单表的系统给迁移到分库分表上去？



**（1）停机迁移方案**

​	最 low 的方案，网站或者app 挂个公告，说 0 点到 6 点进行运维，无法访问......

​	到了 0 点没有流量写入了，此时老的单库单表数据库静止了。然后你之前要写好一个导数的一次性工具，此时直接运行，将单表的数据写到分库分表中。

**缺点：**

​	一定会出现几个小时的停机

​	如果在规定时间内如果没有完成数据迁移，那么就要回滚，基于但表单库继续跑，第二天继续搞



![分布式9](F:\自我总结qwq\消息队列面试题\照片\分布式9.png)



**（2）双写迁移方案**









#### 三.设计可动态扩容的分库分表方案？



1. 选择一个数据库中间件、调研、学习、测试。
2. 设计分库分表的方案，要分成多少库。每个库中分成多少个表
3. 基于选择好得数据库中间件，以及在测试环境建立好得分库分表的环境，测试是否能正常进行分库分表的读写
4. 完成单库单表到分库分表的迁移，双写方案
5. 线上系统开始基于分库分表对外提供服务
6. 再次扩容，扩容成 6 个库，每个库需要 12 个表，怎么来增加更多的库和表。



要在一开始设计的时候，就要弄好分库分表方案！！！

这是比较简单的方案：只进行数据库数量的改变

刚开始的时候，这个库可能就是个逻辑库，建在一个数据库服务器上，一个 MySQL 服务器可能建立 n 个库，后面如果进行拆分，就是不断在库和 MySQL 服务器之间做迁移就可以了，然后系统修改一下配置。

哪怕是减少库的数量，也很简单，就是按倍数缩容就可以了，然后修改下路由规则



1. 设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是 32 库 * 32 表，对于大部分公司，可能几年都够了。
2. 路由的规则 ：orderID % 32 = 库，orderID / 32 % 32 = 表。
3. 扩容的时候，申请增加更多的数据库服务器，装好 MySQL，倍数扩容，4 台服务器，扩到 8 台服务器，16 台服务器。
4. 由 dba 负责将原先数据库服务器的库，迁移到新的数据库服务器上去，很多工具，库迁移，比较方便。
5. 我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址。
6. 重新发布上线，原先的路由规则都不用变，直接基于 2 倍的数据库服务器的资源，继续进行线上系统的提供服务。



![分布式10](F:\自我总结qwq\消息队列面试题\照片\分布式10.png)







#### 四.分库分表后，id 主键如何处理？



**（1）数据库自增 id**

使用方便，缺点：单库生成自增 id，要是高并发的话，就会有瓶颈，硬要改进，专门开一个服务出来，这个服务每次就拿到当前 id 最大值，然后自己递增几个 id，一次性返回一批 id，然后再把当前最大 id 值修改成递增几个 id 之后的一个值，但是无论怎么说都是基于单个数据库。

适合场景：

​	并发低，但是数据量大，所有要靠分库分表来存放海量的数据。

![分布式11](F:\自我总结qwq\消息队列面试题\照片\分布式11.png)



**（2）uuid**

​	好处就是本地生成，不用基于数据库；不好之处就是 uuid 太长了，作为主键性能太差，不适合用于主键，而且在数据库中也不是按顺序排序

适合的场景：

​	随机生成个文件名、编号之类的



**（3）获取系统当前时间**

​	获取当前系统时间即可，问题是，并发很高的情况下，比如 1s 并发几千，会有重复的情况，这个肯定是不合适的

适用场景：

​	如果采用这个方案，是将当前时间和其他业务字段进行拼接起来，作为一个 id，

​	全局唯一编号、订单编号 =  时间戳 + 用户 id + 业务含义编码

**（4）snowflake 算法** 

占用 8 byte（64 bit）

0 | 0001100 10100010 10111110 10001001 01011100 00 | 10001 | 11001 | 0000 00000000

- 0  -->  代表正数：


- 2021-03-19 10:19:23  --> 换算成一个二进制，41 bit存放   0001100 10100010 10111110 10001001 01011100 00

- 机房 id 17  --->  10001
- 机器 id 25  --->  11001

- snowflake 会判断当前请求是否是机房 17 机器 25 在 2021-03-19 10:19:23 这个时间点发送过来的第一个请求,如果是第一个请求 0000 00000000
  - 如果同一个机器在同一毫秒发送多个请求，那么就会 0000 00000000、0000 00000001、0000 00000010 等进行排序





**总结：**

​	机房和机器 id 号都是用 5bit 存放，也就是说机器 id 最多只能是 32 以内。

​	同一毫秒的请求数用 12bit 存放，只能有 4096 个请求，无论你传递多少进来，这个位运算保证始终就是在 4096 这个范围内。


一张表适合存储多少数据：

​	阿里巴巴《Java 开发手册》提出单表行数超过 500 万行或者单表容量超过 2GB，才推荐进行分库分表

一个库适合存储多少数据：

一个 MySQL 实例适合存储多少数据：





## MySQL 读写分离



**为什么 MySQL 要读写分离？**

实际上大部分的互联网公司、一些网站、app 都是读多写少，针对这个情况就是写一个主库，但是主库挂多个从库，从库负责读，可支撑更高的读并发压力。









### 1.主从复制的原理

![mysql1](F:\自我总结qwq\Typora文件\Redis\mysql1.png)



MySQL复制过程分为三步：

​	1、 Master将改变记录到二进制日志(Binary Log)。这些记录过程叫做二进制日志事件，`Binary Log Events`；

​	2、 Slave将Master的`Binary Log Events`拷贝到它的中继日志(Replay Log);

​	3、 Slave重做中继日志中的事件，将改变应用到自己的数据库中。MySQL复制是异步且串行化的。



- Slave 的 I/O Thread 从 Master 中读取 Binary Log 在 **5.6.x 版本**之前是串行，之后支持多线程读取
- Slave 的 SQL Thread 单线程读取 Relay Log，应用日志变更到自己本地数据，串行化。





### 2.主从延时问题的产生原因

**Slave 进行数据同步时一定会导致一个问题**，从库的数据实际上来说一般都会比主库要慢一些。也就是 **主从延迟**

**主从延迟**：产生所谓的主从延迟，主要看主库的写并发。



**一主一从条件下（经验经历）**

1. 主从延时主要看主库的写并发
2. 主库的写并发达到 1000/s，从库的延时会有几 ms。
3. 主库的写并发达到 2000/s，从库的延时会有几十 ms。
4. 主库的写并发达到 4000/s、6000/s，主库都快被打死了，从库延迟可能会达到几秒。



### 3.MySQL的半同步机制、并行复制

主库突然宕机，然后恰好数据还没同步到从库，那么有些数据可能在从库上是没有的，有些数据就丢失了。

MySQL 在这一块有两个机制

- **半同步复制，解决主库数据丢失的问题；**
- **并行复制，解决主从同步延时的问题；**



**semi-sync 半同步机制：**

​	semi-sync 半同步复制，指的就是主库写入 binlog 日志之后，此时就会强制将数据同步到从库，从库将日志写入到自己的 relay log 之后，接着就会返回一个 ack 给主库，主库接收到至少一个从库的 ack 之后才会认为自己的写操作完成了。

- 为什么叫半同步：因为 Slave 只是将 binary log 拉取到自己的 relay log 中，Slave 还没有 写入到自己的数据中



**并行复制：（MySQL5.7 版本）**

​	指的是从库开启多个线程，并行读取 relay log 中不同的日志，然后并行重放不同库的日志，这是库级别的并行

​	就是有多个 SQL 线程去执行 relay log 中的日志，MySQL 5.7 新版本的并行机制。

​	多个 SQL 线程、多个库的 relay log 可以并行的执行



某个库的写入并发特别高，单库写并发达到 2000/s，并行复制还是没意义。



### 4.主从延时导致bug

这块东西经常碰到，就比如用了 MySQL 主从架构之后，可能会发现，刚写入库的数据结果没查到，就完蛋了..........

所以实际上你要考虑好应该在什么场景下来用这个 MySQL 主从同步，建议一般是在读远远多于写，而且读的适合一般对数据的时效性要求没那么高，用主从同步



这个时候可以使用 MySQL 的并行复制，但是问题时 并行复制 是库级别的并行，所以有时候作用不是很大。





一般来说，如果主从延迟较为严重，单主单从，此时主机写并发 2000/s

1. 分库：将一个主库拆分为 4 个主库，每个主库的写并发就 500/s，此时主从延迟可以忽略不计。

2. 打开 MySQL 的并行复制，多个库并行复制，如果说单个库的写并发达到 2000/s，开启并行复制意义不大

   



## Dubbo



Dubbo 的网络通信原理：Netty



如果让你















## 业务问题

每个服务每天多少请求量。高峰期每秒钟多少请求量，完全可以在代码里稍微加一些 metrics（度量）的代码

任何一个开源系统都需要对自己运行过程中各种请求量、每秒的请求量、成功次数、失败次数，在内存里直接做一些计数，它会给你开放一些端口号，比如 http 端口号，你只要请求它这个端口号就会把这些 metrics 统计返回给你



在你负责的核心服务、核心接口，开发一个简单的 metric 统计机制，AtomicLong 原子性，并发下数据统计准确、不会失误，每个接口被调用的时候，一个是可以对每个接口每分钟都做一个 metric 统计。

完全可以通过 log4j、logback、日志组件，把每分钟每个接口被访问的次数直接打印到日志文件里去，在高峰期的每分钟 / 60 = 高峰期每秒访问次数

对每个接口每天的请求：使用一个 AtomicLong 做一个计数，统计出来每天的请求次数

计算一下每个接口从请求到执行完毕需要耗费多长时间，算一下每个接口平均的请求延时

- TP99 = 100ms：99%以上的请求耗费的时间在 100 ms以内，1%以上的请求耗费的时间超过 100ms
- TP95 = 50ms：95%以上的请求耗费的时间在 50 ms以内，1%以上的请求耗费的时间超过 50ms







#### 接口幂等性



跟自己的业务系统有关系，你们的系统服务之间的调用，有没有超时和重试的配置，如果没有，如何优化配置，如果有，核心接口有没有幂等性机制，重复插入数据，重复更新数据。

如果有问题，结合你的业务，如何基于唯一索引、redis 定制化防重机制





分布式事务、分布式锁

核心交易链路、核心数据链路、核心计算链路、核心业务链路，都建议要上分布式事务，保证核心链路的数据一致性







## Seata







亮点：

1、应用层基于 SQL 解析实现了自动补偿，从而最大程度的降低业务侵入性

2、将分布式事务中 TC（事务协调者）独立部署，负责事务的注册、回滚

3、通过全局锁实现了写隔离与读隔离

























